{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance Evaluation\n",
    "Detailed analysis of model performance metrics and cross-validation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import mlflow\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from src.utils.metrics import MetricsCalculator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'experiment_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 21\u001b[0m\n\u001b[1;32m     13\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     14\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m'\u001b[39m: run\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mrun_id,\n\u001b[1;32m     15\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m: run\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     16\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrun\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mmetrics\n\u001b[1;32m     17\u001b[0m         })\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(results)\n\u001b[0;32m---> 21\u001b[0m results_df \u001b[38;5;241m=\u001b[39m load_mlflow_results(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meeg_classification\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m, in \u001b[0;36mload_mlflow_results\u001b[0;34m(experiment_name)\u001b[0m\n\u001b[1;32m      3\u001b[0m client \u001b[38;5;241m=\u001b[39m mlflow\u001b[38;5;241m.\u001b[39mtracking\u001b[38;5;241m.\u001b[39mMlflowClient()\n\u001b[1;32m      4\u001b[0m experiment \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mget_experiment_by_name(experiment_name)\n\u001b[1;32m      6\u001b[0m runs \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39msearch_runs(\n\u001b[0;32m----> 7\u001b[0m     experiment_ids\u001b[38;5;241m=\u001b[39m[experiment\u001b[38;5;241m.\u001b[39mexperiment_id],\n\u001b[1;32m      8\u001b[0m     order_by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetrics.f1_score DESC\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m run \u001b[38;5;129;01min\u001b[39;00m runs:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'experiment_id'"
     ]
    }
   ],
   "source": [
    "def load_mlflow_results(experiment_name):\n",
    "    \"\"\"Load results from MLflow tracking.\"\"\"\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    experiment = client.get_experiment_by_name(experiment_name)\n",
    "    \n",
    "    runs = client.search_runs(\n",
    "        experiment_ids=[experiment.experiment_id],\n",
    "        order_by=['metrics.f1_score DESC']\n",
    "    )\n",
    "    \n",
    "    results = []\n",
    "    for run in runs:\n",
    "        results.append({\n",
    "            'run_id': run.info.run_id,\n",
    "            'model_type': run.data.params.get('model_type'),\n",
    "            **run.data.metrics\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "results_df = load_mlflow_results('eeg_classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_comparison(results):\n",
    "    \"\"\"Plot comparison of key metrics across models.\"\"\"\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for idx, metric in enumerate(metrics):\n",
    "        sns.boxplot(data=results, x='model_type', y=metric, ax=axes[idx])\n",
    "        axes[idx].set_xticklabels(axes[idx].get_xticklabels(), rotation=45)\n",
    "        axes[idx].set_title(f'{metric.replace(\"_\", \" \").title()}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_metrics_comparison(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrices(predictions, model_types):\n",
    "    \"\"\"Plot confusion matrices for different models.\"\"\"\n",
    "    n_models = len(model_types)\n",
    "    fig, axes = plt.subplots(1, n_models, figsize=(5*n_models, 4))\n",
    "    \n",
    "    for ax, model in zip(axes, model_types):\n",
    "        model_preds = predictions[predictions['model_type'] == model]\n",
    "        cm = confusion_matrix(model_preds['true_label'], \n",
    "                            model_preds['predicted_label'])\n",
    "        \n",
    "        sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap='Blues')\n",
    "        ax.set_title(f'{model}')\n",
    "        ax.set_xlabel('Predicted')\n",
    "        ax.set_ylabel('True')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(cv_results):\n",
    "    \"\"\"Plot learning curves from cross-validation results.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for model_type in cv_results['model_type'].unique():\n",
    "        model_results = cv_results[cv_results['model_type'] == model_type]\n",
    "        \n",
    "        plt.plot(model_results['train_size'], \n",
    "                model_results['train_score'], \n",
    "                label=f'{model_type} (train)')\n",
    "        plt.plot(model_results['train_size'], \n",
    "                model_results['val_score'], \n",
    "                label=f'{model_type} (val)')\n",
    "    \n",
    "    plt.xlabel('Training Examples')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Learning Curves')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_prediction_errors(predictions):\n",
    "    \"\"\"Analyze characteristics of prediction errors.\"\"\"\n",
    "    errors = predictions[predictions['true_label'] != predictions['predicted_label']]\n",
    "    \n",
    "    # Error rate by participant\n",
    "    participant_errors = errors.groupby('Participant').size()\n",
    "    participant_total = predictions.groupby('Participant').size()\n",
    "    error_rates = (participant_errors / participant_total).sort_values(ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    error_rates.plot(kind='bar')\n",
    "    plt.title('Error Rate by Participant')\n",
    "    plt.xlabel('Participant')\n",
    "    plt.ylabel('Error Rate')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze feature values for errors\n",
    "    feature_cols = [col for col in predictions.columns \n",
    "                   if col not in ['Participant', 'true_label', 'predicted_label']]\n",
    "    \n",
    "    for feature in feature_cols:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(data=predictions, x='true_label', y=feature, \n",
    "                   hue='predicted_label')\n",
    "        plt.title(f'{feature} Distribution for Correct/Incorrect Predictions')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_threshold_sensitivity(predictions):\n",
    "    \"\"\"Analyze model performance across different probability thresholds.\"\"\"\n",
    "    thresholds = np.linspace(0.1, 0.9, 9)\n",
    "    metrics_calculator = MetricsCalculator()\n",
    "    \n",
    "    results = []\n",
    "    for threshold in thresholds:\n",
    "        pred_labels = (predictions['probability'] >= threshold).astype(int)\n",
    "        metrics = metrics_calculator.calculate_classification_metrics(\n",
    "            predictions['true_label'],\n",
    "            pred_labels\n",
    "        )\n",
    "        results.append({'threshold': threshold, **metrics})\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for metric in ['accuracy', 'precision', 'recall', 'f1_score']:\n",
    "        plt.plot(results_df['threshold'], \n",
    "                results_df[metric], \n",
    "                label=metric)\n",
    "    \n",
    "    plt.xlabel('Classification Threshold')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Metric Scores vs Classification Threshold')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_confidence_distribution(predictions):\n",
    "    \"\"\"Analyze distribution of model confidence scores.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Correct predictions\n",
    "    correct_mask = predictions['true_label'] == predictions['predicted_label']\n",
    "    \n",
    "    sns.kdeplot(data=predictions[correct_mask], x='probability',\n",
    "                label='Correct Predictions')\n",
    "    sns.kdeplot(data=predictions[~correct_mask], x='probability',\n",
    "                label='Incorrect Predictions')\n",
    "    \n",
    "    plt.xlabel('Model Confidence')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Distribution of Model Confidence by Prediction Correctness')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate confidence statistics\n",
    "    confidence_stats = predictions.groupby(\n",
    "        predictions['true_label'] == predictions['predicted_label']\n",
    "    )['probability'].describe()\n",
    "    \n",
    "    print(\"\\nConfidence Statistics:\")\n",
    "    print(confidence_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_temporal_patterns(predictions):\n",
    "    \"\"\"Analyze performance patterns over time/sequence.\"\"\"\n",
    "    predictions['window_idx'] = predictions.groupby('Participant').cumcount()\n",
    "    \n",
    "    # Accuracy over window sequence\n",
    "    window_accuracy = predictions.groupby('window_idx').apply(\n",
    "        lambda x: (x['true_label'] == x['predicted_label']).mean()\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    window_accuracy.plot()\n",
    "    plt.xlabel('Window Sequence')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Model Accuracy Over Window Sequence')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
