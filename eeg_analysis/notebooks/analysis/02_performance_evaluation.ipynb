{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance Evaluation\n",
    "Detailed analysis of model performance metrics and cross-validation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import mlflow\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from src.utils.metrics import MetricsCalculator\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "sns.set_context('talk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mlflow_results(experiment_name):\n",
    "    \"\"\"Load results from MLflow tracking.\"\"\"\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    experiment = client.get_experiment_by_name(experiment_name)\n",
    "    \n",
    "    runs = client.search_runs(\n",
    "        experiment_ids=[experiment.experiment_id],\n",
    "        order_by=['metrics.f1_score DESC']\n",
    "    )\n",
    "    \n",
    "    results = []\n",
    "    for run in runs:\n",
    "        results.append({\n",
    "            'run_id': run.info.run_id,\n",
    "            'model_type': run.data.params.get('model_type'),\n",
    "            **run.data.metrics\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "results_df = load_mlflow_results('eeg_classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_comparison(results):\n",
    "    \"\"\"Plot comparison of key metrics across models.\"\"\"\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for idx, metric in enumerate(metrics):\n",
    "        sns.boxplot(data=results, x='model_type', y=metric, ax=axes[idx])\n",
    "        axes[idx].set_xticklabels(axes[idx].get_xticklabels(), rotation=45)\n",
    "        axes[idx].set_title(f'{metric.replace(\"_\", \" \").title()}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_metrics_comparison(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrices(predictions, model_types):\n",
    "    \"\"\"Plot confusion matrices for different models.\"\"\"\n",
    "    n_models = len(model_types)\n",
    "    fig, axes = plt.subplots(1, n_models, figsize=(5*n_models, 4))\n",
    "    \n",
    "    for ax, model in zip(axes, model_types):\n",
    "        model_preds = predictions[predictions['model_type'] == model]\n",
    "        cm = confusion_matrix(model_preds['true_label'], \n",
    "                            model_preds['predicted_label'])\n",
    "        \n",
    "        sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap='Blues')\n",
    "        ax.set_title(f'{model}')\n",
    "        ax.set_xlabel('Predicted')\n",
    "        ax.set_ylabel('True')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(cv_results):\n",
    "    \"\"\"Plot learning curves from cross-validation results.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for model_type in cv_results['model_type'].unique():\n",
    "        model_results = cv_results[cv_results['model_type'] == model_type]\n",
    "        \n",
    "        plt.plot(model_results['train_size'], \n",
    "                model_results['train_score'], \n",
    "                label=f'{model_type} (train)')\n",
    "        plt.plot(model_results['train_size'], \n",
    "                model_results['val_score'], \n",
    "                label=f'{model_type} (val)')\n",
    "    \n",
    "    plt.xlabel('Training Examples')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Learning Curves')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_prediction_errors(predictions):\n",
    "    \"\"\"Analyze characteristics of prediction errors.\"\"\"\n",
    "    errors = predictions[predictions['true_label'] != predictions['predicted_label']]\n",
    "    \n",
    "    # Error rate by participant\n",
    "    participant_errors = errors.groupby('Participant').size()\n",
    "    participant_total = predictions.groupby('Participant').size()\n",
    "    error_rates = (participant_errors / participant_total).sort_values(ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    error_rates.plot(kind='bar')\n",
    "    plt.title('Error Rate by Participant')\n",
    "    plt.xlabel('Participant')\n",
    "    plt.ylabel('Error Rate')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze feature values for errors\n",
    "    feature_cols = [col for col in predictions.columns \n",
    "                   if col not in ['Participant', 'true_label', 'predicted_label']]\n",
    "    \n",
    "    for feature in feature_cols:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(data=predictions, x='true_label', y=feature, \n",
    "                   hue='predicted_label')\n",
    "        plt.title(f'{feature} Distribution for Correct/Incorrect Predictions')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_threshold_sensitivity(predictions):\n",
    "    \"\"\"Analyze model performance across different probability thresholds.\"\"\"\n",
    "    thresholds = np.linspace(0.1, 0.9, 9)\n",
    "    metrics_calculator = MetricsCalculator()\n",
    "    \n",
    "    results = []\n",
    "    for threshold in thresholds:\n",
    "        pred_labels = (predictions['probability'] >= threshold).astype(int)\n",
    "        metrics = metrics_calculator.calculate_classification_metrics(\n",
    "            predictions['true_label'],\n",
    "            pred_labels\n",
    "        )\n",
    "        results.append({'threshold': threshold, **metrics})\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for metric in ['accuracy', 'precision', 'recall', 'f1_score']:\n",
    "        plt.plot(results_df['threshold'], \n",
    "                results_df[metric], \n",
    "                label=metric)\n",
    "    \n",
    "    plt.xlabel('Classification Threshold')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Metric Scores vs Classification Threshold')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_confidence_distribution(predictions):\n",
    "    \"\"\"Analyze distribution of model confidence scores.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Correct predictions\n",
    "    correct_mask = predictions['true_label'] == predictions['predicted_label']\n",
    "    \n",
    "    sns.kdeplot(data=predictions[correct_mask], x='probability',\n",
    "                label='Correct Predictions')\n",
    "    sns.kdeplot(data=predictions[~correct_mask], x='probability',\n",
    "                label='Incorrect Predictions')\n",
    "    \n",
    "    plt.xlabel('Model Confidence')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Distribution of Model Confidence by Prediction Correctness')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate confidence statistics\n",
    "    confidence_stats = predictions.groupby(\n",
    "        predictions['true_label'] == predictions['predicted_label']\n",
    "    )['probability'].describe()\n",
    "    \n",
    "    print(\"\\nConfidence Statistics:\")\n",
    "    print(confidence_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_temporal_patterns(predictions):\n",
    "    \"\"\"Analyze performance patterns over time/sequence.\"\"\"\n",
    "    predictions['window_idx'] = predictions.groupby('Participant').cumcount()\n",
    "    \n",
    "    # Accuracy over window sequence\n",
    "    window_accuracy = predictions.groupby('window_idx').apply(\n",
    "        lambda x: (x['true_label'] == x['predicted_label']).mean()\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    window_accuracy.plot()\n",
    "    plt.xlabel('Window Sequence')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Model Accuracy Over Window Sequence')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
