# Window-Level Model Configuration

# Data Configuration
data:
  feature_path: "data/processed/features/2s_window_features.parquet"
  split:
    test_size: 0.2
    random_state: 42
    stratify: true

# Model Configuration
model_type: "random_forest" # Options: random_forest, gradient_boosting, logistic_regression, pytorch_mlp, keras_mlp, etc.
# Note: Window-level trainer currently uses single model training only

model:
  name: "patient_model"

  params:
    random_forest:
      n_estimators: 200
      min_samples_leaf: 2 # Smaller than window-level due to fewer samples
      max_features: "sqrt"
      class_weight: "balanced"
      random_state: 42

    gradient_boosting:
      n_estimators: 200
      learning_rate: 0.1
      max_depth: 3
      min_samples_leaf: 1
      random_state: 42

    logistic_regression:
      max_iter: 1000
      class_weight: "balanced"
      penalty: "l2"
      solver: "lbfgs"
      random_state: 42

    logistic_regression_l1:
      max_iter: 1000
      class_weight: "balanced"
      penalty: "l1"
      solver: "liblinear"
      random_state: 42

    svm_rbf:
      kernel: "rbf"
      C: 1.0
      class_weight: "balanced"
      probability: true
      random_state: 42

    svm_linear:
      kernel: "linear"
      C: 1.0
      class_weight: "balanced"
      probability: true
      random_state: 42

    extra_trees:
      n_estimators: 200
      min_samples_leaf: 2
      class_weight: "balanced"
      random_state: 42

    ada_boost:
      n_estimators: 100
      learning_rate: 0.1
      random_state: 42

    knn:
      n_neighbors: 3
      weights: "distance"

    decision_tree:
      min_samples_leaf: 2
      class_weight: "balanced"
      random_state: 42

    sgd:
      loss: "modified_huber"
      penalty: "elasticnet"
      class_weight: "balanced"
      max_iter: 1000
      random_state: 42

# Deep Learning Configuration
deep_learning:
  pytorch_mlp:
    hidden_layers: [64, 32] # Conservative network size
    dropout_rate: 0.3 # Moderate dropout for regularization
    weight_decay: 0.01 # L2 regularization
    learning_rate: 0.001 # Conservative learning rate
    batch_size: 32
    epochs: 200 # Conservative epochs with early stopping
    early_stopping_patience: 20 # Patient early stopping
    batch_norm: true
    activation: "relu"
    optimizer: "adam"
    class_weight: "balanced"
    random_state: 42

  keras_mlp:
    hidden_layers: [64, 32] # Conservative network size
    dropout_rate: 0.3 # Moderate dropout for regularization
    l1_reg: 0.001 # Smaller L1 regularization
    l2_reg: 0.01 # L2 regularization for weight decay
    learning_rate: 0.001 # Conservative learning rate
    batch_size: 32
    epochs: 200 # Conservative epochs with early stopping
    early_stopping_patience: 20 # Patient early stopping
    batch_norm: true
    activation: "relu"
    optimizer: "adam"
    class_weight: "balanced"
    random_state: 42

# Paths Configuration
paths:
  models: "models/window_level"
  features: "data/processed/features"
  logs: "logs"

# Output Configuration
output:
  save_predictions: true
  save_probabilities: true
  feature_importance: true
  performance_plots: true

# MLflow Tracking
mlflow:
  experiment_name: "window_training"
  tracking_uri: "file:./mlruns"
  register_model: true
  model_stage: "Development"
  artifact_location: "./mlruns"

# Evaluation Metrics
metrics:
  window_level:
    - accuracy
    - precision
    - recall
    - f1
    - roc_auc
  patient_level:
    - accuracy
    - precision
    - recall
    - f1
    - roc_auc

# Logging Configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "logs/window_training.log"
  console: true
