# ULTRA-EXTREME GPU CONFIGURATION for Dual RTX 3090
# WARNING: This config pushes hardware beyond normal limits!
# Designed to maximize VRAM usage and GPU utilization

# Data Configuration
data:
  feature_path: "eeg_analysis/data/processed/features/{window_size}s_{channels}_window_features.parquet"
  split:
    test_size: 0.2
    random_state: 42
    stratify: true

# Model Configuration - ULTRA-EXTREME GPU utilization
model_type: "advanced_hybrid_1dcnn_lstm" # OPTIMIZED GPU-accelerated hybrid model

model:
  name: "patient_model_ultra_extreme"
  # Class balancing options (choose only ONE):
  use_smote: false        # Oversampling technique - creates synthetic minority samples
  use_nearmiss: false    # Undersampling technique - removes majority samples (alternative to SMOTE)
  nearmiss_version: 1    # 1, 2, or 3 - different NearMiss strategies (only used if use_nearmiss: true)
  params:
    random_forest:
      n_estimators: 200
      min_samples_leaf: 2
      max_features: "sqrt"
      class_weight: "balanced"
      random_state: 42

    gradient_boosting:
      n_estimators: 200
      learning_rate: 0.1
      max_depth: 3
      min_samples_leaf: 1
      random_state: 42

    xgboost_gpu:
      n_estimators: 500  # More trees for better performance
      max_depth: 8  # Deeper trees
      learning_rate: 0.15  # Faster learning
      tree_method: "hist"  # Modern tree method
      device: "cuda:0"  # GPU acceleration (new API)
      subsample: 0.9  # Slight regularization
      colsample_bytree: 0.9  # Feature sampling
      class_weight: "balanced"  # Handle class imbalance
      random_state: 42

    catboost_gpu:
      iterations: 200  # Optimized for speed
      depth: 6  # Optimized for faster training
      learning_rate: 0.2  # Higher learning rate for fewer iterations
      task_type: "GPU"  # GPU acceleration
      devices: "0"  # Use first GPU
      bootstrap_type: "Bernoulli"  # Better for GPU
      subsample: 0.8  # More aggressive subsampling for speed
      border_count: 128  # Reduced for faster GPU training
      max_ctr_complexity: 1  # Simplified for speed
      gpu_ram_part: 0.8  # Use more GPU memory for performance
      # Note: colsample_bylevel (rsm) not supported in GPU mode for binary classification
      class_weight: "balanced"  # Handle class imbalance
      random_state: 42

    lightgbm_gpu:
      n_estimators: 500  # More estimators
      max_depth: 8  # Deeper trees
      learning_rate: 0.15  # Aggressive learning rate
      device: "gpu"  # GPU acceleration
      gpu_device_id: 0  # Use first GPU
      num_leaves: 255  # Optimized for GPU
      subsample: 0.9  # Regularization
      colsample_bytree: 0.9  # Feature sampling
      class_weight: "balanced"  # Handle class imbalance
      random_state: 42

    logistic_regression:
      max_iter: 1000
      class_weight: "balanced"
      penalty: "l2"
      solver: "lbfgs"
      random_state: 42

    logistic_regression_l1:
      max_iter: 1000
      class_weight: "balanced"
      penalty: "l1"
      solver: "liblinear"
      random_state: 42

    svm_rbf:
      kernel: "rbf"
      C: 1.0
      class_weight: "balanced"
      probability: true
      random_state: 42

    svm_linear:
      kernel: "linear"
      C: 1.0
      class_weight: "balanced"
      probability: true
      random_state: 42

    extra_trees:
      n_estimators: 200
      min_samples_leaf: 2
      class_weight: "balanced"
      random_state: 42

    ada_boost:
      n_estimators: 100
      learning_rate: 0.1
      random_state: 42

    knn:
      n_neighbors: 3
      weights: "distance"

    decision_tree:
      min_samples_leaf: 2
      class_weight: "balanced"
      random_state: 42

    sgd:
      loss: "modified_huber"
      penalty: "elasticnet"
      class_weight: "balanced"
      max_iter: 1000
      random_state: 42

# ULTRA-EXTREME GPU Configuration - MAXIMUM POSSIBLE
deep_learning:
  pytorch_mlp:
    hidden_layers: [4096, 2048, 1024, 512, 256, 128] # ENORMOUS 6-layer network!
    dropout_rate: 0.02 # Extremely minimal dropout for max computation
    weight_decay: 0.000001 # Almost no weight decay
    learning_rate: 0.001 # Aggressive but stable learning rate
    batch_size: 8192 # MAXIMUM batch size - push VRAM to the limit!
    epochs: 25 # Fewer epochs but massive computational load per epoch
    early_stopping_patience: 3 # Very aggressive early stopping
    batch_norm: true
    activation: "relu"
    optimizer: "adam"
    class_weight: "balanced"
    random_state: 42
    # Additional extreme settings
    gradient_accumulation_steps: 1 # Could be increased for even larger effective batch
    mixed_precision: false # Enable for memory efficiency

  keras_mlp:
    hidden_layers: [4096, 2048, 1024, 512, 256, 128] # ENORMOUS 6-layer network!
    dropout_rate: 0.02
    l1_reg: 0.0000001 # Almost no regularization
    l2_reg: 0.000001 # Almost no regularization  
    learning_rate: 0.15 # INSANELY aggressive learning rate
    batch_size: 8192 # MAXIMUM batch size per GPU = 16384 total effective!
    epochs: 25 # Fewer epochs
    early_stopping_patience: 3
    batch_norm: true
    activation: "relu"
    optimizer: "adam"
    class_weight: "balanced"
    random_state: 42
    mixed_precision: true

  hybrid_1dcnn_lstm:
    # Improved architecture for better feature learning and generalization
    cnn_filters: [64, 128, 256]  # Deeper CNN for better feature extraction
    cnn_kernel_sizes: [3, 3, 3]
    cnn_pool_sizes: [2, 2, 2]
    cnn_dropout: 0.3  # Moderate dropout for regularization
    
    # Multi-layer LSTM for better temporal modeling
    lstm_units: [128, 64]  # Two layers with decreasing size
    lstm_dropout: 0.3  # Moderate dropout
    lstm_recurrent_dropout: 0.2  # Moderate recurrent dropout
    
    # Balanced dense layers for better classification
    dense_layers: [128, 64]  # Two dense layers
    dense_dropout: 0.4  # Moderate dropout
    
    # Optimized training parameters
    learning_rate: 0.003  # Standard learning rate
    batch_size: 64  # Moderate batch size
    epochs: 10  # More epochs for better convergence
    early_stopping_patience: 25  # More patience
    
    # Enhanced features
    optimizer: 'adam'
    class_weight: 'balanced'
    random_state: 42
    sequence_length: 1000
    n_channels: 4
    normalize: True
    bidirectional_lstm: True  # Enable bidirectional for better temporal modeling
    attention_mechanism: True  # Enable attention for better feature focus
    residual_connections: True  # Enable residual connections for better gradient flow
    batch_norm: True
    mixed_precision: True
    
    # Balanced regularization
    weight_decay: 0.00001  # Moderate weight decay
    gradient_clip_norm: 1.0  # Standard gradient clipping

  advanced_hybrid_1dcnn_lstm:
    # OPTIMIZED architecture for maximum speed and GPU utilization
    # Set use_cnn: false to disable CNN blocks and use LSTM only
    use_cnn: true  # Set to false to remove CNN blocks (may be redundant with feature extraction)
    cnn_blocks:
      - filters: [64, 64]
        kernel_sizes: [5, 3]  # Optimized kernel sizes
        pool_size: 2
        dilation_rates: [1, 1]
        separable_conv: False  # Disabled for speed
        
      - filters: [128, 128]
        kernel_sizes: [3, 3]  # Simplified kernels
        pool_size: 2
        dilation_rates: [1, 1]  # No dilation for speed
        separable_conv: False
        
      - filters: [256, 128]
        kernel_sizes: [3, 1]  # Final reduction
        pool_size: 1
        dilation_rates: [1, 1]
        separable_conv: False
    
    # Optimized regularization
    normalization: 'batch_norm'  # Faster than layer_norm
    cnn_dropout: [0.1, 0.2, 0.3]  # Progressive dropout
    spatial_dropout: True
    gaussian_noise: 0.005  # Reduced noise for speed
    
    # Simplified LSTM architecture
    lstm_architecture:
      - units: 128
        return_sequences: True
        bidirectional: True
        dropout: 0.2
        recurrent_dropout: 0.1
        
      - units: 64
        return_sequences: False
        bidirectional: True
        dropout: 0.3
        recurrent_dropout: 0.2
    
    # Simplified attention
    attention_config:
      num_heads: 8  # Reduced from 8
      key_dim: 64  # Reduced from 64
      dropout: 0.1
      use_positional_encoding: True  # Disabled for speed
      attention_type: 'multi_head'
    
    # Simplified fusion
    fusion_strategy: 'concat'  # Simpler than concat_attention
    feature_pyramid: True
    
    # Optimized dense architecture
    dense_architecture:
      - units: 256
        activation: 'gelu'  # Faster than swish
        dropout: 0.3
        batch_norm: True
        
      - units: 128
        activation: 'gelu'
        dropout: 0.4
        batch_norm: True
    
    # Optimized training configuration
    optimizer_config:
      name: 'adamw'  # Better than Adam
      learning_rate: 0.001
      weight_decay: 0.01
      beta_1: 0.9
      beta_2: 0.999
      epsilon: 1e-7
    
    # Simplified learning rate schedule
    lr_schedule:
      type: 'cosine_annealing_warm_restarts'
      initial_lr: 0.001
      min_lr: 1e-6
      warmup_epochs: 5  # Reduced from 10
      cycle_length: 20  # Reduced from 30
      cycle_mult: 1  # No multiplication for speed
    
    # Optimized training parameters
    batch_size: 1024  # Increased for better GPU utilization
    epochs: 100  # Reduced from 200
    early_stopping:
      patience: 10  # Reduced from 30
      restore_best_weights: True
      monitor: 'val_f1_macro'
      min_delta: 0.001
    
    # Disabled augmentation for speed
    augmentation:
      time_warping: False
      magnitude_warping: False
      window_slicing: False
      mixup_alpha: 0.0
      cutmix_alpha: 0.0
      
    # Simplified preprocessing
    preprocessing:
      sequence_length: 1000
      n_channels: 4
      normalization: 'standard_scaler'  # Faster than robust_scaler
      filter_noise: False
      apply_wavelet_denoising: False
    
    # Optimized regularization
    regularization:
      label_smoothing: 0.05  # Reduced from 0.1
      gradient_clip_norm: 1.0
      ema_decay: 0.999
      stochastic_weight_averaging: False  # Disabled for speed
      dropout_schedule: 'constant'  # No schedule for speed
    
    # Disabled ensemble for speed
    ensemble:
      num_models: 1  # Single model
      diversity_loss_weight: 0.0
      uncertainty_estimation: False
      monte_carlo_dropout: False
      test_time_augmentation: False
    
    # Simplified loss function
    loss_config:
      primary_loss: 'cross_entropy'  # Faster than focal_loss
      auxiliary_losses: []  # No auxiliary losses
      class_weights: 'balanced'
    
    # Disabled architecture enhancements for speed
    architecture_enhancements:
      squeeze_excitation: False
      coordinate_attention: False
      ghost_convolutions: False
      inverted_residuals: False
      progressive_resizing: False
    
    # Simplified validation
    validation:
      cross_validation_folds: 1  # No CV during training
      stratified_split: True
      time_series_split: False
    
    # Disabled monitoring for speed
    monitoring:
      wandb_logging: False
      tensorboard_logging: False
      model_checkpointing: 'best_f1'
      learning_curves: False
      attention_visualization: False
    
    # Optimized hardware settings
    hardware_optimization:
      mixed_precision: True  # Enable for speed
      xla_compilation: False  # Disabled for compatibility
      memory_growth: True
      distribute_strategy: 'single_gpu'  # Single GPU for simplicity
      
    # Reproducibility
    random_state: 10
    deterministic_training: True  # Disabled for speed
    
    # Disabled post-training for speed
    post_training:
      knowledge_distillation: False
      pruning_ratio: 0.0
      quantization: 'none'
      onnx_export: False

  efficient_tabular_mlp:
    # Lean, high-performance MLP for tabular window features
    hidden_layers: [1024, 512, 256, 128]
    dropout_rate: 0.3
    batch_norm: true
    
    # Training config
    optimizer_config:
      name: 'adamw'
      learning_rate: 0.001
      weight_decay: 0.01
      beta_1: 0.9
      beta_2: 0.999
      epsilon: 1e-7
    lr_schedule:
      type: 'cosine_annealing_warm_restarts'
      initial_lr: 0.001
      min_lr: 1e-6
      cycle_length: 20
      cycle_mult: 1
    regularization:
      label_smoothing: 0.05
      gradient_clip_norm: 1.0
    batch_size: 1024
    epochs: 100
    early_stopping:
      patience: 10
      restore_best_weights: true
      monitor: 'loss'
      min_delta: 0.001
    mixed_precision: true
    class_weight: 'balanced'
    random_state: 42
    
    cnn_blocks:
      - filters: [64, 64]
        kernel_sizes: [5, 3]  # Optimized kernel sizes
        pool_size: 2
        dilation_rates: [1, 1]
        separable_conv: False  # Disabled for speed
        
      - filters: [128, 128]
        kernel_sizes: [3, 3]  # Simplified kernels
        pool_size: 2
        dilation_rates: [1, 1]  # No dilation for speed
        separable_conv: False
        
      - filters: [256, 128]
        kernel_sizes: [3, 1]  # Final reduction
        pool_size: 1
        dilation_rates: [1, 1]
        separable_conv: False
    
    # Optimized regularization
    normalization: 'batch_norm'  # Faster than layer_norm
    cnn_dropout: [0.1, 0.2, 0.3]  # Progressive dropout
    spatial_dropout: True
    gaussian_noise: 0.005  # Reduced noise for speed
    
    # No LSTM architecture (CNN only) - but provide a dummy LSTM to avoid errors
    # Use 4 units to make it divisible by num_heads=4
    lstm_architecture:
      - units: 4  # Changed from 1 to 4 to be divisible by num_heads
        return_sequences: False
        bidirectional: False
        dropout: 0.0
        recurrent_dropout: 0.0
    
    # Simplified attention
    attention_config:
      num_heads: 4  # Reduced from 8
      key_dim: 32  # Reduced from 64
      dropout: 0.1
      use_positional_encoding: False  # Disabled for speed
      attention_type: 'multi_head'
    
    # Simplified fusion
    fusion_strategy: 'concat'  # Simpler than concat_attention
    feature_pyramid: True
    
    # Optimized dense architecture
    dense_architecture:
      - units: 256
        activation: 'relu'  # Faster than swish
        dropout: 0.3
        batch_norm: True
        
      - units: 128
        activation: 'relu'
        dropout: 0.4
        batch_norm: True
    
    # Optimized training configuration
    optimizer_config:
      name: 'adamw'  # Better than Adam
      learning_rate: 0.001
      weight_decay: 0.01
      beta_1: 0.9
      beta_2: 0.999
      epsilon: 1e-7
    
    # Simplified learning rate schedule
    lr_schedule:
      type: 'cosine_annealing_warm_restarts'
      initial_lr: 0.001
      min_lr: 1e-6
      warmup_epochs: 5  # Reduced from 10
      cycle_length: 20  # Reduced from 30
      cycle_mult: 1  # No multiplication for speed
    
    # Optimized training parameters
    batch_size: 1024  # Increased for better GPU utilization
    epochs: 100  # Reduced from 200
    early_stopping:
      patience: 10  # Reduced from 30
      restore_best_weights: True
      monitor: 'val_f1_macro'
      min_delta: 0.001
    
    # Disabled augmentation for speed
    augmentation:
      time_warping: False
      magnitude_warping: False
      window_slicing: False
      mixup_alpha: 0.0
      cutmix_alpha: 0.0
      
    # Simplified preprocessing
    preprocessing:
      sequence_length: 1000
      n_channels: 4
      normalization: 'standard_scaler'  # Faster than robust_scaler
      filter_noise: False
      apply_wavelet_denoising: False
    
    # Optimized regularization
    regularization:
      label_smoothing: 0.05  # Reduced from 0.1
      gradient_clip_norm: 1.0
      ema_decay: 0.999
      stochastic_weight_averaging: False  # Disabled for speed
      dropout_schedule: 'constant'  # No schedule for speed
    
    # Disabled ensemble for speed
    ensemble:
      num_models: 1  # Single model
      diversity_loss_weight: 0.0
      uncertainty_estimation: False
      monte_carlo_dropout: False
      test_time_augmentation: False
    
    # Simplified loss function
    loss_config:
      primary_loss: 'cross_entropy'  # Faster than focal_loss
      auxiliary_losses: []  # No auxiliary losses
      class_weights: 'balanced'
    
    # Disabled architecture enhancements for speed
    architecture_enhancements:
      squeeze_excitation: False
      coordinate_attention: False
      ghost_convolutions: False
      inverted_residuals: False
      progressive_resizing: False
    
    # Simplified validation
    validation:
      cross_validation_folds: 1  # No CV during training
      stratified_split: True
      time_series_split: False
    
    # Disabled monitoring for speed
    monitoring:
      wandb_logging: False
      tensorboard_logging: False
      model_checkpointing: 'best_f1'
      learning_curves: False
      attention_visualization: False
    
    # Optimized hardware settings
    hardware_optimization:
      mixed_precision: True  # Enable for speed
      xla_compilation: False  # Disabled for compatibility
      memory_growth: True
      distribute_strategy: 'single_gpu'  # Single GPU for simplicity
      
    # Reproducibility
    random_state: 42
    deterministic_training: False  # Disabled for speed
    
    # Disabled post-training for speed
    post_training:
      knowledge_distillation: False
      pruning_ratio: 0.0
      quantization: 'none'
      onnx_export: False

  advanced_lstm:
    # No CNN blocks (LSTM only) - but provide a dummy block to avoid errors
    cnn_blocks:
      - filters: [1]  # Dummy single filter
        kernel_sizes: [1]  # Dummy kernel size
        pool_size: 1
        dilation_rates: [1]
        separable_conv: False
    
    # Minimal CNN-specific regularization
    normalization: 'batch_norm'
    cnn_dropout: [0.0]  # No dropout for dummy CNN
    spatial_dropout: False
    gaussian_noise: 0.005
    
    # Simplified LSTM architecture
    lstm_architecture:
      - units: 128
        return_sequences: True
        bidirectional: True
        dropout: 0.2
        recurrent_dropout: 0.1
        
      - units: 64
        return_sequences: False
        bidirectional: True
        dropout: 0.3
        recurrent_dropout: 0.2
    
    # Simplified attention
    attention_config:
      num_heads: 4  # Reduced from 8
      key_dim: 32  # Reduced from 64
      dropout: 0.1
      use_positional_encoding: False  # Disabled for speed
      attention_type: 'multi_head'
    
    # Simplified fusion
    fusion_strategy: 'concat'  # Simpler than concat_attention
    feature_pyramid: True
    
    # Optimized dense architecture
    dense_architecture:
      - units: 256
        activation: 'relu'  # Faster than swish
        dropout: 0.3
        batch_norm: True
        
      - units: 128
        activation: 'relu'
        dropout: 0.4
        batch_norm: True
    
    # Optimized training configuration
    optimizer_config:
      name: 'adamw'  # Better than Adam
      learning_rate: 0.001
      weight_decay: 0.01
      beta_1: 0.9
      beta_2: 0.999
      epsilon: 1e-7
    
    # Simplified learning rate schedule
    lr_schedule:
      type: 'cosine_annealing_warm_restarts'
      initial_lr: 0.001
      min_lr: 1e-6
      warmup_epochs: 5  # Reduced from 10
      cycle_length: 20  # Reduced from 30
      cycle_mult: 1  # No multiplication for speed
    
    # Optimized training parameters
    batch_size: 1024  # Increased for better GPU utilization
    epochs: 100  # Reduced from 200
    early_stopping:
      patience: 10  # Reduced from 30
      restore_best_weights: True
      monitor: 'val_f1_macro'
      min_delta: 0.001
    
    # Disabled augmentation for speed
    augmentation:
      time_warping: False
      magnitude_warping: False
      window_slicing: False
      mixup_alpha: 0.0
      cutmix_alpha: 0.0
      
    # Simplified preprocessing
    preprocessing:
      sequence_length: 1000
      n_channels: 4
      normalization: 'standard_scaler'  # Faster than robust_scaler
      filter_noise: False
      apply_wavelet_denoising: False
    
    # Optimized regularization
    regularization:
      label_smoothing: 0.05  # Reduced from 0.1
      gradient_clip_norm: 1.0
      ema_decay: 0.999
      stochastic_weight_averaging: False  # Disabled for speed
      dropout_schedule: 'constant'  # No schedule for speed
    
    # Disabled ensemble for speed
    ensemble:
      num_models: 1  # Single model
      diversity_loss_weight: 0.0
      uncertainty_estimation: False
      monte_carlo_dropout: False
      test_time_augmentation: False
    
    # Simplified loss function
    loss_config:
      primary_loss: 'cross_entropy'  # Faster than focal_loss
      auxiliary_losses: []  # No auxiliary losses
      class_weights: 'balanced'
    
    # Disabled architecture enhancements for speed
    architecture_enhancements:
      squeeze_excitation: False
      coordinate_attention: False
      ghost_convolutions: False
      inverted_residuals: False
      progressive_resizing: False
    
    # Simplified validation
    validation:
      cross_validation_folds: 1  # No CV during training
      stratified_split: True
      time_series_split: False
    
    # Disabled monitoring for speed
    monitoring:
      wandb_logging: False
      tensorboard_logging: False
      model_checkpointing: 'best_f1'
      learning_curves: False
      attention_visualization: False
    
    # Optimized hardware settings
    hardware_optimization:
      mixed_precision: True  # Enable for speed
      xla_compilation: False  # Disabled for compatibility
      memory_growth: True
      distribute_strategy: 'single_gpu'  # Single GPU for simplicity
      
    # Reproducibility
    random_state: 42
    deterministic_training: False  # Disabled for speed
    
    # Disabled post-training for speed
    post_training:
      knowledge_distillation: False
      pruning_ratio: 0.0
      quantization: 'none'
      onnx_export: False

# Paths Configuration
paths:
  models: "models/window_level"
  features: "eeg_analysis/data/processed/features"
  logs: "logs"

# Output Configuration
output:
  save_predictions: true
  save_probabilities: true
  feature_importance: true
  performance_plots: true

# MLflow Tracking
mlflow:
  experiment_name: "paper1"
  tracking_uri: "file:./mlruns"
  register_model: true
  model_stage: "Development"
  artifact_location: "./mlruns"

# Evaluation Metrics
metrics:
  window_level:
    - accuracy
    - precision
    - recall
    - f1
    - roc_auc
  patient_level:
    - accuracy
    - precision
    - recall
    - f1
    - roc_auc

# Logging Configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "logs/window_training_ultra_extreme.log"
  console: true 